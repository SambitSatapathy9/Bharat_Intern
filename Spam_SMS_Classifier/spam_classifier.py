# -*- coding: utf-8 -*-
"""Spam_Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oSf-PSW1pTOCA1cX6XcSzhX2m-D99NEC

# Spam/Ham Classifier
## Step-by-step Analysis
1. Import Necessary Dependencies
2. Import Dataset and Data Handling
3. Text Preprocessing
   - Stemming
   - Using Stopwords
4. Feature Extraction (Vectorisation)
    - C-BOW
    - TF-IDF
5. Model Building and evaluating
6. Predicting the model
7. Performance metrics and accuracy score.
8. Conclusion
"""

# Mount Google Drive to access files
# from google.colab import drive
# drive.mount('/content/drive')

# Upload files from local system
# from google.colab import files
# uploaded = files.upload()

# List files in the current directory
# import os
# os.listdir('.')

"""## 1. Import necessary dependencies"""

import numpy as np
import pandas as pd #for data handling

#For model building, and performance metrics
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# for text preprocessing
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

import re #regular expression

"""## 2. Import the Dataset
- The dataset is a tab-separated spam classification text where we will be implementing NLP methods like stemming, stopwords and Word embedding techniques.
- We have two columns, output- label and input- message.
"""

# Reading a text file with Pandas
df = pd.read_csv('SMSSpamCollection.txt', sep='\t',names= ['label','message'])
df

"""## Data Handling"""

df.shape

df.info()

df.describe()

df.isnull().sum() #There are no null values in the dataset.

df['message'].iloc[1554], df['label'].iloc[1554]

#Separating the dataframe into input features (messages) and output labels
X = df['message']
y = df['label']
len(X), len(y)  # Displaying the length of X (messages) and y (labels)

# Convert the labels 'spam' to 1 and 'ham' to 0 in the variable y
y = [1 if label == 'spam' else 0 for label in y]

"""## 3. Text Preprocessing
### 3.1 Stemming and Using stopwords
"""

# nltk.download("stopwords")

# ps = PorterStemmer()
lemm = WordNetLemmatizer()
stop_words = stopwords.words("english")

# nltk.download('wordnet')

corpus = []
for i in range(len(X)):
    text = re.sub('[^a-zA-Z0-9]',' ',X[i]) #Preprocessing the text data by removing special characters
    text = text.lower() # Text is converted to lowercase #
    text = text.split() #Text is split into individual words.

    text = [lemm.lemmatize(word) for word in text if word not in stop_words]  #Lemmatization is applied to obtain the base forms of words, and stopwords are removed from the text
    text = " ".join(text) # The cleaned text is then joined back into sentences
    corpus.append(text) #The cleaned text is appended to the 'corpus' list.

corpus # The 'corpus' variable contains the preprocessed text data.

"""## 4. Feature Extraction
### 4.1 Vectorisation
"""

#calculate the lengths of sentences in the corpus list
sent_lengths = [len(corpus[i].split(" ")) for i in range(len(corpus))]
print(','.join(map(str, sent_lengths))) #print the length of the sentences in one line with comma as a separable

print(np.max(sent_lengths))

"""#### 4.1.1 Bag-Of-Words Model"""

#Creating a Bag-of-words model using CountVectoriser
cbow = CountVectorizer(binary=True,ngram_range=(1,2))
X_cbow = cbow.fit_transform(corpus).toarray()

#Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_cbow,y,test_size=0.25,random_state=68)

"""##### 4.1.1.1 Using Multinomial Naive Bayes (MultinomialNB)"""

#Model Building
cbow_model = MultinomialNB()
cbow_model.fit(X_train,y_train) #Fit the model
y_pred = cbow_model.predict(X_test) #Model Prediction

#Performance Metrics and Accuracy Score
score = accuracy_score(y_pred,y_test)
print(f"Accuracy Score for C-BOW Model using Multinomial Naive Bayes: {score}")

print(f"Classification Report: \n {classification_report(y_pred,y_test)}")

print(confusion_matrix(y_test,y_pred))

"""###### 4.1.1.2 Random Forest Classifier
Using Random Forest Classifier instead of Multinomial Naive Bayes (MultinomialNB)
"""

rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)

#Performance Metrics and Accuracy Score
score = accuracy_score(y_pred,y_test)
print(f"Accuracy Score for C-BOW Model using RF Classifier:\t {score}")

print(f"Classification Report: \n {classification_report(y_pred,y_test)}")

print(f"Confusion Matrix: \n {confusion_matrix(y_test,y_pred)}")

"""##### 4.1.2 TF-IDF VECTORISER"""

tf = TfidfVectorizer(ngram_range=(1,2),binary=True)
X_tf = tf.fit_transform(corpus).toarray()

#Train-test Split
X_train,X_test,y_train,y_test = train_test_split(X_tf,y,test_size=0.25, random_state=68)

"""##### 4.1.2.1 Using Multinomial Naive Bayes (MultinomialNB)"""

#5. Model Building
tf_model = MultinomialNB()
tf_model.fit(X_train,y_train)

#6. Prediction of the model
y_pred = tf_model.predict(X_test)

#7. Performance Metrics and Accuracy Score
score = accuracy_score(y_pred,y_test)
print(f"Accuracy Score for TF-IDF Model using Multinomial Naive Bayes: {score}")

print(f"Classification Report: \n {classification_report(y_pred,y_test)}")

print(f"Confusion Matrix: \n {confusion_matrix(y_test,y_pred)}")

"""##### 4.1.2.2 Using Random Forest Classifier
Using Random Forest Classifier instead of Multinomial Naive Bayes (MultinomialNB)
"""

from sklearn.ensemble import RandomForestClassifier

#5. Model Building
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
#6. Model Prediction
y_pred = rf_model.predict(X_test)

#7. Performance Metrics and Accuracy Score
score = accuracy_score(y_pred,y_test)
print(f"Accuracy Score for TF-IDF Model using RF Classifier:\t {score}")

print(f"Classification Report: \n {classification_report(y_pred,y_test)}")

print(f"Confusion Matrix: \n {confusion_matrix(y_test,y_pred)}")

"""# 8. Conclusion
Accuracy Score for C-BOW Model:
1. Using Multinomial Naive Bayes - 97.41%
2. Using RF Classifier - 97.48%

Accuracy Score for TF-IDF Model:
1. Using Multinomial Naive Bayes - 95.33 %
2. Using RF Classifier - 97.91%
"""



